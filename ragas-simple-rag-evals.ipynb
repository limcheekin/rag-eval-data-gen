{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09169c4",
   "metadata": {},
   "source": [
    "https://docs.ragas.io/en/stable/getstarted/rag_eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dfb4e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb018f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "                        model=os.getenv(\"GENERATOR_MODEL\"),\n",
    "                        api_key=os.getenv(\"GENERATOR_API_KEY\"),\n",
    "                        base_url=os.getenv(\"GENERATOR_BASE_URL\")\n",
    "                    )\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "                        model=os.getenv(\"EMBEDDER_MODEL\"),\n",
    "                        api_key=os.getenv(\"EMBEDDER_API_KEY\"),\n",
    "                        base_url=os.getenv(\"EMBEDDER_BASE_URL\")\n",
    "                    )\n",
    "        self.doc_embeddings = None\n",
    "        self.docs = None\n",
    "\n",
    "    def load_documents(self, documents):\n",
    "        \"\"\"Load documents and compute their embeddings.\"\"\"\n",
    "        self.docs = documents\n",
    "        self.doc_embeddings = self.embeddings.embed_documents(documents)\n",
    "\n",
    "    def get_most_relevant_docs(self, query):\n",
    "        \"\"\"Find the most relevant document for a given query.\"\"\"\n",
    "        if not self.docs or not self.doc_embeddings:\n",
    "            raise ValueError(\"Documents and their embeddings are not loaded.\")\n",
    "\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        similarities = [\n",
    "            np.dot(query_embedding, doc_emb)\n",
    "            / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "            for doc_emb in self.doc_embeddings\n",
    "        ]\n",
    "        most_relevant_doc_index = np.argmax(similarities)\n",
    "        return [self.docs[most_relevant_doc_index]]\n",
    "\n",
    "    def generate_answer(self, query, relevant_doc):\n",
    "        \"\"\"Generate an answer for a given query based on the most relevant document.\"\"\"\n",
    "        prompt = f\"question: {query}\\n\\nDocuments: {relevant_doc}\"\n",
    "        messages = [\n",
    "            (\"system\", \"You are a helpful assistant that answers questions based on given documents only.\"),\n",
    "            (\"user\", prompt),\n",
    "        ]\n",
    "        ai_msg = self.llm.invoke(messages)\n",
    "        return ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98778ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"docs/\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae79dc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text Chunkers initialized.\n",
      "\n",
      "ğŸ”„ Chunking documents...\n",
      "ğŸ‘ Documents split into 46 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"), \n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False,  # Keep headers in content for context\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True,\n",
    ")\n",
    "print(\"âœ… Text Chunkers initialized.\")\n",
    "\n",
    "print(\"\\nğŸ”„ Chunking documents...\")\n",
    "md_split_docs = []\n",
    "for doc in docs:\n",
    "    md_header_splits = markdown_splitter.split_text(doc.page_content)\n",
    "    md_header_splits = text_splitter.split_documents(md_header_splits)\n",
    "    for split_chunk in md_header_splits:\n",
    "        md_split_docs.append(split_chunk.page_content)\n",
    "print(f\"ğŸ‘ Documents split into {len(md_split_docs)} chunks.\")\n",
    "docs = md_split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0143db6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã€Šç¥å¥‡ä¹‹é—¨: å¥‡é—¨éç”²å¤§è§£è°œã€‹  \n",
      "å¼ å¿—æ˜¥è‘—  \n",
      "ä¸Š ç¼–  \n",
      "æ˜“å­¦æ€ç»´ä¸­çš„ç§‘å­¦æ€§ç²¾å  \n",
      "ç¬¬ä¸€ç«   \n",
      "æ˜“å­¦çš„ç§‘å­¦ä»·å€¼ä¸»è¦åœ¨æ€ç»´ç§‘å­¦  \n",
      "1. æ˜“å­¦çš„å†å²å‘å±•  \n",
      "èµ·æºä¸æ¼”å˜ï¼šä»ä¼ç¾²ç”»å…«å¦å¼€å§‹ï¼Œå†ç»å¤ã€Šè¿å±±ã€‹ã€å•†ã€Šå½’è—ã€‹ã€å‘¨ã€Šå‘¨æ˜“ã€‹çš„æˆå‹ï¼Œåˆ°å­”å­ä½œ\"åç¿¼\"ï¼ˆæ˜“ä¼ ï¼‰ï¼Œæ±‰ä»£åˆ—ä¸ºç¾¤ç»ä¹‹é¦–ï¼Œå½¢æˆå®Œæ•´çš„æ˜“å­¦ä½“ç³»ã€‚  \n",
      "æµæ´¾åˆ†åŒ–ï¼šæ¸…ä»£ã€Šå››åº“å…¨ä¹¦ã€‹æ¦‚æ‹¬æ˜“å­¦ä¸º\"ä¸¤æ´¾å…­å®—\"------è±¡æ•°æ´¾ï¼ˆå«è±¡æ•°ã€æœºç¥¥ã€å›¾ä¹¦ï¼‰ä¸ä¹‰ç†æ´¾ï¼ˆå«å„’ç†ã€è€ƒå²ï¼‰ï¼Œä¸¤è€…é•¿æœŸäº‰è®ºå¹¶å­˜ï¼Œæ¶µç›–å“²å­¦ã€å¤©æ–‡ã€å†›äº‹ç­‰å¹¿æ³›é¢†åŸŸã€‚  \n",
      "2. æ˜“å­¦çš„ç°ä»£ç ”ç©¶  \n",
      "å…¨çƒå½±å“ï¼šè¿‘ä»£ä»¥æ¥ï¼Œæ˜“å­¦ä»ä¸­å›½èµ°å‘ä¸–ç•Œï¼Œå¼•å‘è·¨å­¦ç§‘ç ”ç©¶çƒ­æ½®ï¼ŒåŒ…æ‹¬ç¤¾ä¼šç§‘å­¦ã€è‡ªç„¶ç§‘å­¦çš„å¤šå…ƒè§†è§’ã€‚  \n",
      "ç§‘å­¦è¯„ä»·ï¼šä¸­å¤–å­¦è€…ï¼ˆå¦‚é‚“æ‹“ã€è£æ ¼ã€å¼ åå’Œï¼‰é«˜åº¦è¯„ä»·ã€Šå‘¨æ˜“ã€‹çš„æ™ºæ…§ä»·å€¼ï¼Œè®¤ä¸ºå…¶å®‡å®™è§‚ä¸è§„å¾‹æ€§ç ”ç©¶å¯¹ç°ä»£ç§‘å­¦ï¼ˆå¦‚é‡å­ç‰©ç†ï¼‰æœ‰å¯å‘ï¼Œç”šè‡³ä¸è¯ºè´å°”å¥–æˆæœç›¸å…³ã€‚  \n",
      "3. æ˜“å­¦çš„ç§‘å­¦ä»·å€¼äº‰è®®  \n",
      "è§‚ç‚¹åˆ†æ­§ï¼š  \n",
      "éƒ¨åˆ†äººè®¤ä¸ºä»·å€¼ä»…åœ¨\"æ˜“ç†\"ï¼ˆå“²å­¦ï¼‰ï¼Œå¦å®š\"è±¡æ•°/æ•°æœ¯\"çš„ç§‘å­¦æ€§ï¼›  \n",
      "å¦ä¸€æ´¾è®¤ä¸º\"è±¡æ•°\"æ˜¯æ˜“ç†çš„åº”ç”¨ä½“ç°ï¼Œä¸¤è€…ä¸å¯å‰²è£‚ï¼›  \n",
      "æŠ˜ä¸­è§‚ç‚¹ä¸»å¼ \"å­¦\"ï¼ˆç†è®ºï¼‰ä¸\"æœ¯\"ï¼ˆåº”ç”¨ï¼‰ç»“åˆï¼Œå¤ä¸ºä»Šç”¨ã€‚  \n",
      "ä½œè€…ç«‹åœºï¼šæ”¯æŒç¬¬ä¸‰ç§è§‚ç‚¹ï¼Œå¼ºè°ƒè±¡ã€æ•°ã€ç†ã€å æ˜¯ç»Ÿä¸€æ•´ä½“ï¼Œæ˜“å­¦ä»·å€¼éœ€ä»ç†è®ºå’Œåº”ç”¨ä¸¤æ–¹é¢ç»¼åˆç ”ç©¶ã€‚  \n",
      "4. æ˜“å­¦çš„ç§‘å­¦å®šä½  \n",
      "æ€ç»´ç§‘å­¦ä¸ºæ ¸å¿ƒï¼šä½œè€…å¼•ç”¨é’±å­¦æ£®çš„ç°ä»£ç§‘å­¦ä½“ç³»åˆ†ç±»ï¼Œæå‡ºæ˜“å­¦åº”å½’å…¥æ€ç»´ç§‘å­¦ï¼Œå› å…¶æä¾›äº†ç‹¬ç‰¹çš„è®¤çŸ¥æ¨¡å¼ï¼ˆå¦‚è¾©è¯ã€è±¡æ•°æ€ç»´ï¼‰ï¼Œå¯¹è‡ªç„¶ç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦åŠäººç”Ÿå®è·µå‡æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚  \n",
      "å®è·µä»·å€¼ï¼šæ˜“å­¦ä¸ä»…åŒ…å«å“²å­¦æ€æƒ³ï¼ˆåŸºç¡€ç§‘å­¦å±‚æ¬¡ï¼‰ï¼Œä¹Ÿé€šè¿‡æ•°æœ¯æŠ€æœ¯ï¼ˆå·¥ç¨‹æŠ€æœ¯å±‚æ¬¡ï¼‰è”ç³»å®é™…ï¼Œä¿ƒè¿›ç‰©è´¨ä¸ç²¾ç¥æ–‡æ˜å‘å±•ã€‚  \n",
      "5. ç»“è®º  \n",
      "æ˜“å­¦ä½œä¸ºä¸­åæ–‡åŒ–çš„æ™ºæ…§æ€»æ±‡ï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äºæ€ç»´ç§‘å­¦é¢†åŸŸï¼Œå…¼å…·ç†è®ºæ·±åº¦ä¸å®è·µæ„ä¹‰ï¼Œéœ€ä»¥å¼€æ”¾æ€åº¦ç»§æ‰¿å‘å±•ï¼ŒæœåŠ¡äºç°ä»£ç§‘å­¦ä¸æ–‡æ˜å»ºè®¾ã€‚  \n",
      "ç¬¬äºŒç«   \n",
      "äººè„‘æ€ç»´æ–¹å¼ä¸æ˜“å­¦çš„äº§ç”Ÿ  \n",
      "1. äººç±»æ€ç»´æ–¹å¼çš„åˆ†ç±»  \n",
      "ç°ä»£æ€ç»´ç§‘å­¦å°†äººç±»æ€ç»´æ–¹å¼åˆ†ä¸ºä¸‰ç±»ï¼š  \n",
      "é€»è¾‘æ€ç»´ï¼ˆç†æ€§æ€ç»´ï¼‰ï¼šä»£è¡¨å­¦ç§‘å¦‚æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€‚  \n",
      "å½¢è±¡æ€ç»´ï¼ˆæ„Ÿæ€§æ€ç»´ï¼‰ï¼šä»£è¡¨å­¦ç§‘å¦‚æ–‡å­¦ã€è‰ºæœ¯ã€éŸ³ä¹ã€‚  \n",
      "çµæ„Ÿæ€ç»´ï¼ˆæ„Ÿåº”æ€ç»´/ç›´è§‰æ€ç»´ï¼‰ï¼šä»£è¡¨å­¦ç§‘å¦‚æ˜“å­¦ã€ä½›å­¦ã€å¿ƒç†å­¦ã€æ°”åŠŸã€‚  \n",
      "äººç±»æ€ç»´å‘å±•çš„è§„å¾‹ï¼š  \n",
      "å©´å¹¼å„¿ä»¥æ„Ÿæ€§æ€ç»´ä¸ºä¸»ï¼Œéšç€å¹´é¾„å¢é•¿ï¼Œé€»è¾‘æ€ç»´é€æ¸å¢å¼ºã€‚\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48d9bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ä¸­å›½å¤ä»£æ™ºæ…§ï¼Œç‰¹åˆ«æ˜¯æ˜“å­¦ï¼Œä¸ç°ä»£ç§‘å­¦æ€æƒ³æœ‰ä½•å…³è”ï¼Ÿä»¥å…·ä½“çš„ä¾‹å­æ¥è¯æ˜è¿™ç§å…³è”ã€‚\n",
      "Relevant Document: ['ã€Šç¥å¥‡ä¹‹é—¨: å¥‡é—¨éç”²å¤§è§£è°œã€‹  \\nå¼ å¿—æ˜¥è‘—  \\nä¸Š ç¼–  \\næ˜“å­¦æ€ç»´ä¸­çš„ç§‘å­¦æ€§ç²¾å  \\nç¬¬ä¸€ç«   \\næ˜“å­¦çš„ç§‘å­¦ä»·å€¼ä¸»è¦åœ¨æ€ç»´ç§‘å­¦  \\n1. æ˜“å­¦çš„å†å²å‘å±•  \\nèµ·æºä¸æ¼”å˜ï¼šä»ä¼ç¾²ç”»å…«å¦å¼€å§‹ï¼Œå†ç»å¤ã€Šè¿å±±ã€‹ã€å•†ã€Šå½’è—ã€‹ã€å‘¨ã€Šå‘¨æ˜“ã€‹çš„æˆå‹ï¼Œåˆ°å­”å­ä½œ\"åç¿¼\"ï¼ˆæ˜“ä¼ ï¼‰ï¼Œæ±‰ä»£åˆ—ä¸ºç¾¤ç»ä¹‹é¦–ï¼Œå½¢æˆå®Œæ•´çš„æ˜“å­¦ä½“ç³»ã€‚  \\næµæ´¾åˆ†åŒ–ï¼šæ¸…ä»£ã€Šå››åº“å…¨ä¹¦ã€‹æ¦‚æ‹¬æ˜“å­¦ä¸º\"ä¸¤æ´¾å…­å®—\"------è±¡æ•°æ´¾ï¼ˆå«è±¡æ•°ã€æœºç¥¥ã€å›¾ä¹¦ï¼‰ä¸ä¹‰ç†æ´¾ï¼ˆå«å„’ç†ã€è€ƒå²ï¼‰ï¼Œä¸¤è€…é•¿æœŸäº‰è®ºå¹¶å­˜ï¼Œæ¶µç›–å“²å­¦ã€å¤©æ–‡ã€å†›äº‹ç­‰å¹¿æ³›é¢†åŸŸã€‚  \\n2. æ˜“å­¦çš„ç°ä»£ç ”ç©¶  \\nå…¨çƒå½±å“ï¼šè¿‘ä»£ä»¥æ¥ï¼Œæ˜“å­¦ä»ä¸­å›½èµ°å‘ä¸–ç•Œï¼Œå¼•å‘è·¨å­¦ç§‘ç ”ç©¶çƒ­æ½®ï¼ŒåŒ…æ‹¬ç¤¾ä¼šç§‘å­¦ã€è‡ªç„¶ç§‘å­¦çš„å¤šå…ƒè§†è§’ã€‚  \\nç§‘å­¦è¯„ä»·ï¼šä¸­å¤–å­¦è€…ï¼ˆå¦‚é‚“æ‹“ã€è£æ ¼ã€å¼ åå’Œï¼‰é«˜åº¦è¯„ä»·ã€Šå‘¨æ˜“ã€‹çš„æ™ºæ…§ä»·å€¼ï¼Œè®¤ä¸ºå…¶å®‡å®™è§‚ä¸è§„å¾‹æ€§ç ”ç©¶å¯¹ç°ä»£ç§‘å­¦ï¼ˆå¦‚é‡å­ç‰©ç†ï¼‰æœ‰å¯å‘ï¼Œç”šè‡³ä¸è¯ºè´å°”å¥–æˆæœç›¸å…³ã€‚  \\n3. æ˜“å­¦çš„ç§‘å­¦ä»·å€¼äº‰è®®  \\nè§‚ç‚¹åˆ†æ­§ï¼š  \\néƒ¨åˆ†äººè®¤ä¸ºä»·å€¼ä»…åœ¨\"æ˜“ç†\"ï¼ˆå“²å­¦ï¼‰ï¼Œå¦å®š\"è±¡æ•°/æ•°æœ¯\"çš„ç§‘å­¦æ€§ï¼›  \\nå¦ä¸€æ´¾è®¤ä¸º\"è±¡æ•°\"æ˜¯æ˜“ç†çš„åº”ç”¨ä½“ç°ï¼Œä¸¤è€…ä¸å¯å‰²è£‚ï¼›  \\næŠ˜ä¸­è§‚ç‚¹ä¸»å¼ \"å­¦\"ï¼ˆç†è®ºï¼‰ä¸\"æœ¯\"ï¼ˆåº”ç”¨ï¼‰ç»“åˆï¼Œå¤ä¸ºä»Šç”¨ã€‚  \\nä½œè€…ç«‹åœºï¼šæ”¯æŒç¬¬ä¸‰ç§è§‚ç‚¹ï¼Œå¼ºè°ƒè±¡ã€æ•°ã€ç†ã€å æ˜¯ç»Ÿä¸€æ•´ä½“ï¼Œæ˜“å­¦ä»·å€¼éœ€ä»ç†è®ºå’Œåº”ç”¨ä¸¤æ–¹é¢ç»¼åˆç ”ç©¶ã€‚  \\n4. æ˜“å­¦çš„ç§‘å­¦å®šä½  \\næ€ç»´ç§‘å­¦ä¸ºæ ¸å¿ƒï¼šä½œè€…å¼•ç”¨é’±å­¦æ£®çš„ç°ä»£ç§‘å­¦ä½“ç³»åˆ†ç±»ï¼Œæå‡ºæ˜“å­¦åº”å½’å…¥æ€ç»´ç§‘å­¦ï¼Œå› å…¶æä¾›äº†ç‹¬ç‰¹çš„è®¤çŸ¥æ¨¡å¼ï¼ˆå¦‚è¾©è¯ã€è±¡æ•°æ€ç»´ï¼‰ï¼Œå¯¹è‡ªç„¶ç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦åŠäººç”Ÿå®è·µå‡æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚  \\nå®è·µä»·å€¼ï¼šæ˜“å­¦ä¸ä»…åŒ…å«å“²å­¦æ€æƒ³ï¼ˆåŸºç¡€ç§‘å­¦å±‚æ¬¡ï¼‰ï¼Œä¹Ÿé€šè¿‡æ•°æœ¯æŠ€æœ¯ï¼ˆå·¥ç¨‹æŠ€æœ¯å±‚æ¬¡ï¼‰è”ç³»å®é™…ï¼Œä¿ƒè¿›ç‰©è´¨ä¸ç²¾ç¥æ–‡æ˜å‘å±•ã€‚  \\n5. ç»“è®º  \\næ˜“å­¦ä½œä¸ºä¸­åæ–‡åŒ–çš„æ™ºæ…§æ€»æ±‡ï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äºæ€ç»´ç§‘å­¦é¢†åŸŸï¼Œå…¼å…·ç†è®ºæ·±åº¦ä¸å®è·µæ„ä¹‰ï¼Œéœ€ä»¥å¼€æ”¾æ€åº¦ç»§æ‰¿å‘å±•ï¼ŒæœåŠ¡äºç°ä»£ç§‘å­¦ä¸æ–‡æ˜å»ºè®¾ã€‚  \\nç¬¬äºŒç«   \\näººè„‘æ€ç»´æ–¹å¼ä¸æ˜“å­¦çš„äº§ç”Ÿ  \\n1. äººç±»æ€ç»´æ–¹å¼çš„åˆ†ç±»  \\nç°ä»£æ€ç»´ç§‘å­¦å°†äººç±»æ€ç»´æ–¹å¼åˆ†ä¸ºä¸‰ç±»ï¼š  \\né€»è¾‘æ€ç»´ï¼ˆç†æ€§æ€ç»´ï¼‰ï¼šä»£è¡¨å­¦ç§‘å¦‚æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€‚  \\nå½¢è±¡æ€ç»´ï¼ˆæ„Ÿæ€§æ€ç»´ï¼‰ï¼šä»£è¡¨å­¦ç§‘å¦‚æ–‡å­¦ã€è‰ºæœ¯ã€éŸ³ä¹ã€‚  \\nçµæ„Ÿæ€ç»´ï¼ˆæ„Ÿåº”æ€ç»´/ç›´è§‰æ€ç»´ï¼‰ï¼šä»£è¡¨å­¦ç§‘å¦‚æ˜“å­¦ã€ä½›å­¦ã€å¿ƒç†å­¦ã€æ°”åŠŸã€‚  \\näººç±»æ€ç»´å‘å±•çš„è§„å¾‹ï¼š  \\nå©´å¹¼å„¿ä»¥æ„Ÿæ€§æ€ç»´ä¸ºä¸»ï¼Œéšç€å¹´é¾„å¢é•¿ï¼Œé€»è¾‘æ€ç»´é€æ¸å¢å¼ºã€‚']\n",
      "Answer: æ ¹æ®æä¾›çš„æ–‡ä»¶ï¼Œã€Šç¥å¥‡ä¹‹é—¨: å¥‡é—¨éç”²å¤§è§£è°œã€‹ä¸€ä¹¦æåˆ°äº†ä¸­å›½å¤ä»£æ™ºæ…§ï¼Œç‰¹åˆ«æ˜¯æ˜“å­¦ï¼Œä¸ç°ä»£ç§‘å­¦æ€æƒ³çš„å…³è”ã€‚å…·ä½“æ¥è¯´ï¼š\n",
      "\n",
      "*   **æ˜“å­¦çš„ç§‘å­¦å®šä½ï¼š** ä½œè€…è®¤ä¸ºæ˜“å­¦åº”å½’å…¥æ€ç»´ç§‘å­¦ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸€ç§ç‹¬ç‰¹çš„è®¤çŸ¥æ¨¡å¼ï¼Œå¦‚è¾©è¯ã€è±¡æ•°æ€ç»´ï¼Œå¯¹è‡ªç„¶ç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦åŠäººç”Ÿå®è·µå‡æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚\n",
      "\n",
      "*   **æ€ç»´æ–¹å¼çš„åˆ†ç±»ï¼š** ç°ä»£æ€ç»´ç§‘å­¦å°†äººç±»æ€ç»´æ–¹å¼åˆ†ä¸ºé€»è¾‘æ€ç»´ã€å½¢è±¡æ€ç»´å’Œçµæ„Ÿæ€ç»´ã€‚æ˜“å­¦è¢«å½’ç±»ä¸ºçµæ„Ÿæ€ç»´çš„ä»£è¡¨å­¦ç§‘ä¹‹ä¸€ã€‚\n",
      "\n",
      "*   **æ˜“å­¦çš„ç°ä»£ç ”ç©¶ï¼š** è¿‘ä»£ä»¥æ¥ï¼Œæ˜“å­¦èµ°å‘ä¸–ç•Œï¼Œå¼•å‘è·¨å­¦ç§‘ç ”ç©¶çƒ­æ½®ï¼ŒåŒ…æ‹¬ç¤¾ä¼šç§‘å­¦ã€è‡ªç„¶ç§‘å­¦çš„å¤šå…ƒè§†è§’ã€‚ä¸­å¤–å­¦è€…é«˜åº¦è¯„ä»·ã€Šå‘¨æ˜“ã€‹çš„æ™ºæ…§ä»·å€¼ï¼Œè®¤ä¸ºå…¶å®‡å®™è§‚ä¸è§„å¾‹æ€§ç ”ç©¶å¯¹ç°ä»£ç§‘å­¦ï¼ˆå¦‚é‡å­ç‰©ç†ï¼‰æœ‰å¯å‘ã€‚\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG instance\n",
    "rag = RAG()\n",
    "\n",
    "# Load documents\n",
    "rag.load_documents(docs)\n",
    "\n",
    "# Query and retrieve the most relevant document\n",
    "query = \"ä¸­å›½å¤ä»£æ™ºæ…§ï¼Œç‰¹åˆ«æ˜¯æ˜“å­¦ï¼Œä¸ç°ä»£ç§‘å­¦æ€æƒ³æœ‰ä½•å…³è”ï¼Ÿä»¥å…·ä½“çš„ä¾‹å­æ¥è¯æ˜è¿™ç§å…³è”ã€‚\"\n",
    "relevant_doc = rag.get_most_relevant_docs(query)\n",
    "\n",
    "# Generate an answer\n",
    "answer = rag.generate_answer(query, relevant_doc)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Relevant Document: {relevant_doc}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f2ef53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference'], len=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "df = pd.read_parquet(\"data/ragas_openai_testset.parquet\")\n",
    "df[\"retrieved_contexts\"] = df[\"reference_contexts\"]\n",
    "df[\"response\"] = df[\"reference\"]\n",
    "data_list = df.to_dict(orient=\"records\")\n",
    "evaluation_dataset = EvaluationDataset.from_list(data_list)\n",
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "783564a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/30 [01:12<00:48,  2.85s/it]Exception raised in Job[19]: RateLimitError(Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '18s'}]}}])\n",
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [02:12<00:32,  3.29s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '53s'}]}}])\n",
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 22/30 [02:17<00:23,  2.96s/it]Exception raised in Job[10]: TimeoutError()\n",
      "Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [03:00<01:42, 14.68s/it]Exception raised in Job[13]: TimeoutError()\n",
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 25/30 [03:03<00:43,  8.69s/it]Exception raised in Job[16]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:58<00:00,  7.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.9444, 'factual_correctness(mode=f1)': 1.0000}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(rag.llm)\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "result = evaluate(dataset=evaluation_dataset,\n",
    "                  metrics=[\n",
    "                    LLMContextRecall(), \n",
    "                    Faithfulness(), \n",
    "                    FactualCorrectness()],\n",
    "                  llm=evaluator_llm\n",
    "                )\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09acf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
